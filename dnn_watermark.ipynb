{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mathematical-catalyst",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\aniruddha halder\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:463: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\aniruddha halder\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:464: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\aniruddha halder\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\aniruddha halder\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:466: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\aniruddha halder\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:467: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "# rest of the code\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import sklearn.metrics as metrics\n",
    "import keras.callbacks as callbacks\n",
    "import keras.utils.np_utils as kutils\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "minute-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "labeled-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ipynb.fs.full.<wide_residual_network> import *\n",
    "import ipynb.fs.full.wide_residual_network as wrn\n",
    "from ipynb.fs.full.watermark_regularizers import  WatermarkRegularizer\n",
    "from ipynb.fs.full.watermark_regularizers import  get_wmark_regularizers\n",
    "#from ipynb.fs.full.watermark_regularizers import  show_encoded_mark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rational-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULT_PATH ='./result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "damaged-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_CHKPOINT_FNAME = os.path.join(RESULT_PATH, 'WRN-Weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "absent-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_hdf5(fname, path, data):\n",
    "#     store = pd.HDFStore(fname)\n",
    "    \n",
    "#     if path in store.keys():\n",
    "#         store.remove(path)\n",
    "#     store.append(path,data)\n",
    "#     store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dynamic-uganda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_wmark_signatures(prefix, model):\n",
    "#     for layer_id, wmark_regularizer in get_wmark_regularizers(model):\n",
    "#         fname_w = prefix + '_layer{}_w.npy'.format(layer_id)\n",
    "#         fname_b = prefix + '_layer{}_b.npy'.format(layer_id)\n",
    "#         np.save(fname_w, wmark_regularizer.get_matrix())\n",
    "#         np.sabe(fname_b, wmark_regularizer.get_signature())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "decreased-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_schedule =[60,120,120]\n",
    "# #epoch_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "detailed-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def schedule(epoch_idx):\n",
    "#     if(epoch_idx+1) < lr_schedule[0]:\n",
    "#         return 0.1\n",
    "#     elif(epoch_idx+1) < lr_schedule[1]:\n",
    "#         return 0.02\n",
    "#     elif(epoch_idx+1) < lr_schedule[2]:\n",
    "#         return 0.004\n",
    "    \n",
    "#     return 0.0008\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sunrise-domain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.isdir(RESULT_PATH):\n",
    "#         os.makedirs(RESULT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "guilty-sugar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (trainX, trainY), (testX, testY) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "numerical-organizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(trainX[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "continuous-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainX = trainX.astype('float32')\n",
    "# trainX /= 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "framed-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testX = testX.astype('float32')\n",
    "# testX /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "difficult-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainY = kutils.to_categorical(trainY)\n",
    "# testY = kutils.to_categorical(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "frank-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = ImageDataGenerator(rotation_range=10,\n",
    "#                               width_shift_range=5./32,\n",
    "#                               height_shift_range=5./32,\n",
    "#                               horizontal_flip=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "above-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator.fit(trainX, seed=0, augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "pleased-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     if 'replace_train_y' in train_settings and len(train_settings['replace_train_y']) > 0:\n",
    "#         print('trainY was replaced from \"{}\"'.format(train_settings['replace_train_y']))\n",
    "#         trainY = np.load(train_settings['replace_train_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "worldwide-flexibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "# nb_epoch = 3\n",
    "# scale = 0.01\n",
    "# embed_dim = 256\n",
    "# N = 1\n",
    "# k = 4\n",
    "# target_blk_id = 1\n",
    "# base_modelw_fname = \"\"\n",
    "# wtype = 'random'\n",
    "# randseed = 'none'\n",
    "# ohist_fname = \"result/train_history.h5\"\n",
    "# hist_hdf_path = 'WTYPE_{}/DIM{}/SCALE{}/N{}k{}B{}EPOCH{}/TBLK{}'.format(\n",
    "#     wtype, embed_dim, scale, N, k, batch_size, nb_epoch, target_blk_id)\n",
    "# modelname_prefix = os.path.join(RESULT_PATH, 'wrn_' + hist_hdf_path.replace('/', '_'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "municipal-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = np.ones((1, embed_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "superb-underwear",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wmark_regularizer = WatermarkRegularizer(scale, b, wtype=wtype, randseed=randseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "turned-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_shape = (3, 32, 32) if K.image_dim_ordering() == 'th' else (32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dominant-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = wrn.create_wide_residual_network(init_shape, nb_classes=10, N=N, k=k, dropout=0.00,\n",
    "#                                         wmark_regularizer=wmark_regularizer, target_blk_num=target_blk_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fourth-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "wrong-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Watermark matrix:\\n{}'.format(wmark_regularizer.get_matrix()))\n",
    "# b.shape[2]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "sized-instrumentation",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "compact-conflict",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "modular-reader",
   "metadata": {},
   "source": [
    "##Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "restricted-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgd = SGD(lr=0.1, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "elder-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"acc\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "perceived-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(base_modelw_fname) > 0:\n",
    "#     model.load_weights(base_modelw_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "straight-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Fnish compile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "clinical-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.gray()\n",
    "# plt.imshow(trainX[5],  interpolation='nearest', aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "lonely-leather",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.argv[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "unusual-fever",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "weighted-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_wmark_regularizers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aging-blood",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#   hist = \\\n",
    "#     model.fit_generator(generator.flow(trainX, trainY, batch_size=batch_size), samples_per_epoch=len(trainX), nb_epoch=nb_epoch,\n",
    "#                         callbacks=[callbacks.ModelCheckpoint(MODEL_CHKPOINT_FNAME, monitor=\"val_acc\", save_best_only=True),\n",
    "#                                    LearningRateScheduler(schedule=schedule)\n",
    "#                         ],\n",
    "#                         validation_data=(testX, testY),\n",
    "#                         nb_val_samples=testX.shape[0],)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-member",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target regularizer(1, 0)\n",
      "Wide Residual Network-10-4 created.\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 32, 32, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 32, 32, 16)    448         input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNorma (None, 32, 32, 16)    32          convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 32, 32, 16)    0           batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 32, 32, 64)    9280        activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 32, 32, 64)    128         convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 32, 32, 64)    0           batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 32, 32, 64)    36928       activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_3 (BatchNorma (None, 32, 32, 64)    128         convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 32, 32, 64)    1088        activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 32, 32, 64)    0           batchnormalization_3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 32, 32, 64)    0           convolution2d_2[0][0]            \n",
      "                                                                   activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 16, 16, 64)    0           merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 16, 16, 128)   73856       maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_4 (BatchNorma (None, 16, 16, 128)   256         convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 16, 16, 128)   0           batchnormalization_4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 16, 16, 128)   147584      activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_5 (BatchNorma (None, 16, 16, 128)   256         convolution2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 16, 16, 128)   8320        maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 16, 16, 128)   0           batchnormalization_5[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "merge_2 (Merge)                  (None, 16, 16, 128)   0           convolution2d_5[0][0]            \n",
      "                                                                   activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 8, 8, 128)     0           merge_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 8, 8, 256)     295168      maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_6 (BatchNorma (None, 8, 8, 256)     512         convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 8, 8, 256)     0           batchnormalization_6[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 8, 8, 256)     590080      activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_7 (BatchNorma (None, 8, 8, 256)     512         convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 8, 8, 256)     33024       maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 8, 8, 256)     0           batchnormalization_7[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "merge_3 (Merge)                  (None, 8, 8, 256)     0           convolution2d_8[0][0]            \n",
      "                                                                   activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "averagepooling2d_1 (AveragePooli (None, 1, 1, 256)     0           merge_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 256)           0           averagepooling2d_1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 10)            2570        flatten_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1200170\n",
      "____________________________________________________________________________________________________\n",
      "Watermark matrix:\n",
      "[[-0.1413772   0.40592833  1.43253443 ...  1.56396163  0.28702483\n",
      "  -0.92685877]\n",
      " [-2.16928103 -0.18226819  0.98363871 ... -1.20253886  1.33032893\n",
      "  -2.19361387]\n",
      " [-1.39245293  0.07879488 -0.21556267 ...  1.95659779  0.71471507\n",
      "  -0.03949446]\n",
      " ...\n",
      " [-0.19343781  0.04094118  1.16669151 ... -1.22958074  0.38897984\n",
      "  -1.55906874]\n",
      " [-1.15600269  0.04553051 -0.28385609 ... -1.18056368 -1.19716916\n",
      "   1.32784605]\n",
      " [-0.33158556  1.6503577  -0.12035332 ...  0.61269211  0.60565894\n",
      "   1.77614519]]\n",
      "Finished compiling\n",
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "RESULT_PATH = './result'\n",
    "MODEL_CHKPOINT_FNAME = os.path.join(RESULT_PATH, 'WRN-Weights.h5')\n",
    "\n",
    "def update_hdf5(fname, path, data):\n",
    "    store = pd.HDFStore(fname)\n",
    "\n",
    "    if path in store.keys():\n",
    "        store.remove(path)\n",
    "    store.append(path, data)\n",
    "    store.close()\n",
    "\n",
    "def save_wmark_signatures(prefix, model):\n",
    "    for layer_id, wmark_regularizer in get_wmark_regularizers(model):\n",
    "        fname_w = prefix + '_layer{}_w.npy'.format(layer_id)\n",
    "        fname_b = prefix + '_layer{}_b.npy'.format(layer_id)\n",
    "        np.save(fname_w, wmark_regularizer.get_matrix())\n",
    "        np.save(fname_b, wmark_regularizer.get_signature())\n",
    "\n",
    "lr_schedule = [60, 120, 160]  # epoch_step\n",
    "\n",
    "def schedule(epoch_idx):\n",
    "    if (epoch_idx + 1) < lr_schedule[0]:\n",
    "        return 0.1\n",
    "    elif (epoch_idx + 1) < lr_schedule[1]:\n",
    "        return 0.02 # lr_decay_ratio = 0.2\n",
    "    elif (epoch_idx + 1) < lr_schedule[2]:\n",
    "        return 0.004\n",
    "    return 0.0008\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     settings_json_fname = sys.argv[1]\n",
    "#     train_settings = json.load(open(settings_json_fname))\n",
    "    \n",
    "    if not os.path.isdir(RESULT_PATH):\n",
    "        os.makedirs(RESULT_PATH)\n",
    "        \n",
    "#     # load dataset and fitting data for learning\n",
    "#     if train_settings['dataset'] == 'cifar10':\n",
    "#         dataset = cifar10\n",
    "#         nb_classes = 10\n",
    "#     else:\n",
    "#         print('not supported dataset \"{}\"'.format(train_settings['dataset']))\n",
    "#         exit(1)\n",
    "\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    trainX = trainX.astype('float32')\n",
    "    trainX /= 255.0\n",
    "    testX = testX.astype('float32')\n",
    "    testX /= 255.0\n",
    "    trainY = kutils.to_categorical(trainY)\n",
    "    testY = kutils.to_categorical(testY)\n",
    "\n",
    "    generator = ImageDataGenerator(rotation_range=10,\n",
    "                                   width_shift_range=5./32,\n",
    "                                   height_shift_range=5./32,\n",
    "                                   horizontal_flip=True)\n",
    "    generator.fit(trainX, seed=0, augment=True)\n",
    "\n",
    "#     if 'replace_train_y' in train_settings and len(train_settings['replace_train_y']) > 0:\n",
    "#         print('trainY was replaced from \"{}\"'.format(train_settings['replace_train_y']))\n",
    "#         trainY = np.load(train_settings['replace_train_y'])\n",
    "\n",
    "    # read parameters\n",
    "    batch_size = 64\n",
    "    nb_epoch = 3\n",
    "    scale = 0.01\n",
    "    embed_dim = 256\n",
    "    N = 1\n",
    "    k = 4\n",
    "    target_blk_id = 1\n",
    "    base_modelw_fname = \"\"\n",
    "    wtype = 'random'\n",
    "    randseed = 'none'\n",
    "    ohist_fname = \"result/train_history.h5\"\n",
    "    hist_hdf_path = 'WTYPE_{}/DIM{}/SCALE{}/N{}k{}B{}EPOCH{}/TBLK{}'.format(\n",
    "        wtype, embed_dim, scale, N, k, batch_size, nb_epoch, target_blk_id)\n",
    "    modelname_prefix = os.path.join(RESULT_PATH, 'wrn_' + hist_hdf_path.replace('/', '_'))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    " \n",
    "\n",
    "#     hist_hdf_path = 'WTYPE_{}/DIM{}/SCALE{}/N{}K{}B{}EPOCH{}/TBLK{}'.format(\n",
    "#         wtype, embed_dim, scale, N, k, batch_size, nb_epoch, target_blk_id)\n",
    "#     modelname_prefix = os.path.join(RESULT_PATH, 'wrn_' + hist_hdf_path.replace('/', '_'))\n",
    "\n",
    "    # initialize process for Watermark\n",
    "    b = np.ones((1, embed_dim))\n",
    "    wmark_regularizer = WatermarkRegularizer(scale, b, wtype=wtype, randseed=randseed)\n",
    "\n",
    "    init_shape = (3, 32, 32) if K.image_dim_ordering() == 'th' else (32, 32, 3)\n",
    "    model = wrn.create_wide_residual_network(init_shape, nb_classes=10, N=N, k=k, dropout=0.00,\n",
    "                                             wmark_regularizer=wmark_regularizer, target_blk_num=target_blk_id)\n",
    "    model.summary()\n",
    "    print('Watermark matrix:\\n{}'.format(wmark_regularizer.get_matrix()))\n",
    "\n",
    "    # training process\n",
    "    sgd = SGD(lr=0.1, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"acc\"])\n",
    "    if len(base_modelw_fname) > 0:\n",
    "        model.load_weights(base_modelw_fname)\n",
    "    print(\"Finished compiling\")\n",
    "\n",
    "    hist = \\\n",
    "    model.fit_generator(generator.flow(trainX, trainY, batch_size=batch_size), samples_per_epoch=len(trainX), nb_epoch=nb_epoch,\n",
    "                        callbacks=[callbacks.ModelCheckpoint(MODEL_CHKPOINT_FNAME, monitor=\"val_acc\", save_best_only=True),\n",
    "                                   LearningRateScheduler(schedule=schedule)\n",
    "                        ],\n",
    "                        validation_data=(testX, testY),\n",
    "                        nb_val_samples=testX.shape[0],)\n",
    "#     show_encoded_wmark(model)\n",
    "\n",
    "#     # validate training accuracy\n",
    "#     yPreds = model.predict(testX)\n",
    "#     yPred = np.argmax(yPreds, axis=1)\n",
    "#     yPred = kutils.to_categorical(yPred)\n",
    "#     yTrue = testY\n",
    "\n",
    "#     accuracy = metrics.accuracy_score(yTrue, yPred) * 100\n",
    "#     error = 100 - accuracy\n",
    "#     print(\"Accuracy : \", accuracy)\n",
    "#     print(\"Error : \", error)\n",
    "\n",
    "#     # write history and model parameters to file\n",
    "#     update_hdf5(ohist_fname, hist_hdf_path, pd.DataFrame(hist.history))\n",
    "#     model.save_weights(modelname_prefix + '.weight')\n",
    "\n",
    "#     # write watermark matrix and embedded signature to file\n",
    "#     if target_blk_id > 0:\n",
    "#         save_wmark_signatures(modelname_prefix, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_encoded_wmark(model)\n",
    "\n",
    "#     # validate training accuracy\n",
    "#     yPreds = model.predict(testX)\n",
    "#     yPred = np.argmax(yPreds, axis=1)\n",
    "#     yPred = kutils.to_categorical(yPred)\n",
    "#     yTrue = testY\n",
    "\n",
    "#     accuracy = metrics.accuracy_score(yTrue, yPred) * 100\n",
    "#     error = 100 - accuracy\n",
    "#     print(\"Accuracy : \", accuracy)\n",
    "#     print(\"Error : \", error)\n",
    "\n",
    "#     # write history and model parameters to file\n",
    "#     update_hdf5(ohist_fname, hist_hdf_path, pd.DataFrame(hist.history))\n",
    "#     model.save_weights(modelname_prefix + '.weight')\n",
    "\n",
    "#     # write watermark matrix and embedded signature to file\n",
    "#     if target_blk_id > 0:\n",
    "#         save_wmark_signatures(modelname_prefix, model)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
